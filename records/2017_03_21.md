# 東台資料

目前共有 4 個資料集：

- 第五軸 ABL TDA 168000轉 1267孔，空轉
- 第五軸 ABL TDA 168000轉 1267孔，實際加工
- 第五軸ABL主機板 168000轉 10016孔，空轉
- 第五軸ABL主機板 168000轉 10016孔，實際加工

## Preprocess

### 1. Merge

資料共分 6 個類型：`x`, `y`, `z`, `u`, `v`, `w`，每一類型都按照時間順序分散在[幾十份檔案](../tongtai/assets/data)內，為方便後續操作，先把同類型的分散檔案合為一份，對每一個資料集都產生 6 份檔案：

``` bash
/tongtai/src/preprocess $ python merge.py
```

### 2. Join

6 份檔案一共對應著 6 條時間軸，對後續操作非常不便，因此透過 Join 的動作同步時間軸，將 6 份檔案合為一份，使一個資料集最終對應到 1 個檔案，並且按照 Timestamp 由小至大排序：

``` bash
/tongtai/src/preprocess $ python join.py
```

此步驟使用的是 Pandas 內建的 Join，因為使用了 Library，造成了幾個問題：

- `消耗大量記憶體`

  我一次性讀入 6 個 csv 檔案，存入 6 個 Pandas 的 Dataframe 資料結構，並且建立一個用來輸出結果的 Dataframe，想等運算完畢，再將結果一次寫入輸出的 csv 檔案。過程中使用了 7 個 Dataframe，而且跑的是 Join 運算，因此大量了消耗記憶體

- `找不到適合的演算法，徒增運算時間`

  6 份檔案都是按時間由小到大排序過的，若使用 Join 演算法會浪費了資料已經排序過的特性，複雜度會高達 O(n<sup>6</sup>)，因此嘗試執行此程式後得到極差的效果：

  - 5 分鐘的 Raw Data（約 30 萬筆）需耗費 88 分鐘前處理
  - 10 分鐘的 Raw Data（約 60 萬筆）需耗費 > 3774 分鐘前處理

  後面推測電腦記憶體耗盡，導致重開機，程式被迫中斷

### 3. Fast Join

由於 Library 造成的困擾過多，因此決定直接使用原生的 Python 讀寫檔，搭配改良過的演算法，數分鐘內即可處理完全部 4 份資料集

```
/tongtai/src/preprocess $ python fast_join.py
```

以下表格呈現每個檔案資料筆數，以及 Join 後檔案筆數與占比

| 資料集 | x.csv | y.csv | z.csv | u.csv | v.csv | w.csv | 平均 | Join 後檔案 | 有效資料比例 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 1267孔，空轉 | 294,594 | 294,594 | 294,594 | 297,949 | 297,949 | 297,949 | 296,272 | 294,594 | 99.43% |
| 1267孔，實際加工 | 602,670 | 602,670 | 602,670 | 601,228 | 601,228 | 601,228 | 601,949 | 299,050 | 49.68% |
| 10016孔，空轉 | 2,892,280 | 2,892,280 | 2,892,280 | 2,889,120 | 2,254,857 | 1,444,560 | 2,544,230 | 360,832 | 14.18% |
| 10016孔，實際加工 | 3,415,762 | 3,308,041 | 3,308,041 | 6,404,378 | 6,204,351 | 6,104,344 | 4,790,820 | 1,703,672 | 35.56% |

### 4. Label

  TBD.

  ```
  /tongtai/src/preprocess $ python label.py
  ```
